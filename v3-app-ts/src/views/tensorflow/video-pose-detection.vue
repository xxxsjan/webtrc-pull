<template>
  <div>
    <canvas id="output" :width="width" :height="height"></canvas>
    <video id="video" playsinline autoplay :width="width" :height="height" v-show="false"></video>
  </div>
</template>

<script lang="ts" setup>
import * as poseDetection from '@tensorflow-models/pose-detection';
import '@tensorflow/tfjs-backend-webgl';
import { ref, onMounted } from 'vue';

onMounted(() => {
  init();
});
// å…¶ä»–åœ°æ–¹è¦ç”¨åˆ°çš„å…¬å…±å˜é‡
let videoRef: HTMLVideoElement | HTMLImageElement | HTMLCanvasElement;
let canvasRef: HTMLCanvasElement;
let canvasCtx: CanvasRenderingContext2D;
let detector: PoseDetector;
let model = poseDetection.SupportedModels.PoseNet;
const width = 640,
  height = 480;

// åˆå§‹åŒ–
const init = async () => {
  canvasRef = document.getElementById('output') as HTMLCanvasElement;
  canvasCtx = canvasRef.getContext('2d')!;

  videoRef = document.getElementById('video') as HTMLVideoElement;
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: false,
    video: true,
  });
  videoRef.srcObject = stream;

  // åŠ è½½æ¨¡åž‹
  detector = await poseDetection.createDetector(model, {
    // modelType: 'full',
    quantBytes: 4,
    architecture: 'MobileNetV1',
    outputStride: 16,
    inputResolution: { width: width, height: height },
    multiplier: 0.75,
  });
  // å¼€å§‹æ£€æµ‹
  detectPose();
};

// å¼€å§‹æ£€æµ‹
const detectPose = async () => {
  // èŽ·å–æ£€æµ‹ç»“æžœ
  const poses = await detector.estimatePoses(videoRef, {
    flipHorizontal: false, // æ˜¯å¦æ°´å¹³ç¿»è½¬
    maxPoses: 1, // æœ€å¤§æ£€æµ‹äººæ•°
    // scoreThreshold: 0.5, // ç½®ä¿¡åº¦
    // nmsRadius: 20, // éžæžå¤§å€¼æŠ‘åˆ¶
  });
  // å°† pose ä¸Šçš„ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯å­˜å…¥ pointList
  const pointList = poses[0]?.keypoints || [];
  // console.log('ðŸš€ðŸš€ðŸš€ / pointList', pointList[0]);

  // ç»˜åˆ¶è§†é¢‘
  canvasCtx.drawImage(videoRef, 0, 0, canvasRef.width, canvasRef.height);

  // å°†è¿™ 17 ä¸ªå…³é”®ç‚¹çš„åæ ‡ä¿¡æ¯ ç”»åˆ° canvas ä¸Š

  // ç”»å‡ºæ‰€æœ‰å…³é”®ç‚¹
  pointList.forEach(({ x, y, score, name }: any) => {
    if (score > 0.5) {
      // ç”»ç‚¹
      drawPoint(x, y, 5, '#f00000', canvasCtx);
    }
  });

  // èŽ·å–ç›¸é‚»çš„å…³é”®ç‚¹ä¿¡æ¯
  const adjacentPairs = poseDetection.util.getAdjacentPairs(model);
  // ç”»å‡ºæ‰€æœ‰è¿žçº¿
  adjacentPairs.forEach(([i, j]: any) => {
    const kp1 = pointList[i];
    const kp2 = pointList[j];
    // score ä¸ä¸ºç©ºå°±ç”»çº¿
    const score1 = kp1.score != null ? kp1.score : 1;
    const score2 = kp2.score != null ? kp2.score : 1;
    if (score1 >= 0.5 && score2 >= 0.5) {
      // ç”»å‡ºæ‰€æœ‰è¿žçº¿
      drawSegment([kp1.x, kp1.y], [kp2.x, kp2.y], 'aqua', 1, canvasCtx);
    }
  });

  requestAnimationFrame(() => detectPose());
  // setTimeout(() => {
  //   detectPose();
  // }, 1000 / 60);
};

// ç”»ç‚¹
function drawPoint(x: number, y: number, r: number, color: string, ctx: CanvasRenderingContext2D) {
  ctx.beginPath();
  // x, y, radius, startAngle, endAngle, anticlockwise
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}
// ç”»çº¿æ®µ
function drawSegment(
  [ax, ay]: number[],
  [bx, by]: number[],
  color: string,
  scale: number,
  ctx: CanvasRenderingContext2D
) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = 4;
  ctx.strokeStyle = color;
  ctx.stroke();
}
</script>

<style scoped></style>
